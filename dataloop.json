{
  "displayName": "Active Learning Zvi",
  "name": "active-learning-zvi",
  "scope": "project",
  "version": "1.3.137",
  "codebase": {
    "type": "git",
    "gitUrl": "https://github.com/zvi-dataloop/active-learning-zvi",
    "gitTag": "main"
  },
  "components": {
    "panels": [
      {
        "name": "dataSplit",
        "icon": "icon-dl-qa-sampling",
        "supportedSlots": [
          {
            "type": "pipelineNodeConfig",
            "configuration": {}
          }
        ],
        "conditions": {
          "resources": []
        }
      },
      {
        "name": "newModel",
        "icon": "icon-dl-models",
        "supportedSlots": [
          {
            "type": "pipelineNodeConfig",
            "configuration": {}
          }
        ],
        "conditions": {
          "resources": []
        }
      },
      {
        "name": "evaluateModel",
        "icon": "icon-dl-status-filled",
        "supportedSlots": [
          {
            "type": "pipelineNodeConfig",
            "configuration": {}
          }
        ],
        "conditions": {
          "resources": []
        }
      },
      {
        "name": "compareModels",
        "icon": "icon-dl-compare",
        "supportedSlots": [
          {
            "type": "pipelineNodeConfig",
            "configuration": {}
          }
        ],
        "conditions": {
          "resources": []
        }
      }
    ],
    "pipelineNodes": [
      {
        "panel": "dataSplit",
        "invoke": {
          "type": "function",
          "namespace": "active-learning-nodes.custom_nodes.data_split"
        },
        "scope": "global",
        "categories": [
          "data"
        ]
      },
      {
        "panel": "newModel",
        "invoke": {
          "type": "function",
          "namespace": "model-setup.create_new_model.new_model"
        },
        "scope": "project",
        "categories": [
          "models"
        ]
      },
      {
        "panel": "evaluateModel",
        "invoke": {
          "type": "function",
          "namespace": "model-evaluation.evaluate_model.evaluate_model"
        },
        "scope": "global",
        "categories": [
          "models"
        ]
      },
      {
        "panel": "compareModels",
        "invoke": {
          "type": "function",
          "namespace": "model-comparison.compare_models.compare_models"
        },
        "scope": "project",
        "categories": [
          "models"
        ]
      }
    ],
    "modules": [
      {
        "name": "custom_nodes",
        "versions": {
          "dtlpy": "1.78.17"
        },
        "entryPoint": "modules/main.py",
        "className": "ServiceRunner",
        "initInputs": [],
        "functions": [
          {
            "name": "data_split",
            "description": "The 'Data Split' node is a powerful data processing tool that allows you to dynamically split your data into multiple groups at runtime. Whether you need to sample items for QA tasks or divide your ground truth into train/test/validation sets, the Data Split node makes it easy.\n \n Simply define your groups, set their distribution, and tag each item with its assigned group using a metadata field. Use the Data Split node at any point in your pipeline to customize your data processing.",
            "input": [
              {
                "type": "Item",
                "name": "item"
              }
            ],
            "output": [
              {
                "type": "Item",
                "name": "item"
              }
            ],
            "displayIcon": "qa-sampling",
            "displayName": "Data Split"
          }
        ]
      },
      {
        "name": "create_new_model",
        "versions": {
          "dtlpy": "1.78.17"
        },
        "entryPoint": "modules/new_model.py",
        "className": "model_creator",
        "initInputs": [],
        "functions": [
          {
            "name": "new_model",
            "description": "The Create New Model node generates a new model version by cloning an existing model, making it ready for fine-tuning.\n\n The node requires the following inputs: Base model to clone (train/deployed models only), model configurations for training (JSON), Dataset and its subsets (DQL filters): Train and validation.  The node inputs can be provided using parameters (fixed values or dynamic variables) or through node connections.\n\nUpon execution, the node will generate the new model as output. For more information, see Dataloop's Active Learning documentation.",
            "input": [
              {
                "name": "base_model",
                "type": "Model"
              },
              {
                "name": "dataset",
                "type": "Dataset"
              },
              {
                "name": "train_subset",
                "type": "Json"
              },
              {
                "name": "validation_subset",
                "type": "Json"
              },
              {
                "name": "model_configuration",
                "type": "Json"
              }
            ],
            "output": [
              {
                "name": "new_model",
                "type": "Model"
              }
            ],
            "displayIcon": "models",
            "displayName": "Create New Model"
          }
        ]
      },
      {
        "name": "evaluate_model",
        "versions": {
          "dtlpy": "1.78.17"
        },
        "entryPoint": "modules/model_evaluation.py",
        "className": "evaluator",
        "initInputs": [],
        "functions": [
          {
            "name": "evaluate_model",
            "description": "The 'Evaluate Model' node creates predictions for a given Test set (Dataset and DQL filter) and compares them against the model's ground truth annotations. Scores are generated based on the annotation types and are subsequently uploaded to the platform for further use, such as in model comparison.\n\nBy default, the annotation type(s) considered in the evaluation process are defined by the model output type, 'model.output_type'.\nThe following annotations types are supported: classification, bounding box, polygon, segmentation, and point.\nScores encompass label agreement score, attribute agreement score, and geometry score (e.g. IOU).\nThe node requires the following inputs: Model (train/deployed), Dataset and Test subset (DQL filter).\n\nFor more information, see Dataloop's Active Learning documentation.",
            "input": [
              {
                "name": "model",
                "type": "Model"
              },
              {
                "name": "test_dataset",
                "type": "Dataset"
              },
              {
                "name": "test_filter",
                "type": "Json"
              }
            ],
            "output": [
              {
                "name": "model",
                "type": "Model"
              },
              {
                "name": "test_dataset",
                "type": "Dataset"
              }
            ],
            "displayIcon": "status-filled",
            "displayName": "Evaluate Model"
          }
        ]
      },
      {
        "name": "compare_models",
        "versions": {
          "dtlpy": "1.78.17"
        },
        "entryPoint": "modules/model_compare.py",
        "className": "comparator",
        "initInputs": [],
        "functions": [
          {
            "name": "compare_models",
            "description": "The 'Compare Models' node undertakes a comparison between two trained model versions.\n\nThe default Dataloop compare model node can compare any two models that have either:\n- uploaded metrics to model management during model training, or\n- been evaluated on a common test subset.\n\nThe node requires the following inputs: 'compare_config' (JSON), 'Previous model' and 'New model'. The new model undergoes testing, and if it proves superior (based on the 'compare_config'), it will be sent as an output labelled 'Update model', signifying deployment readiness. Alternatively, it will be labelled 'Discard'. To do the comparison, a compare_config JSON must be provided. For more information, see Dataloop's Active Learning documentation.",
            "input": [
              {
                "name": "previous_model",
                "type": "Model"
              },
              {
                "name": "new_model",
                "type": "Model"
              },
              {
                "name": "compare_config",
                "type": "Json"
              },
              {
                "name": "dataset",
                "type": "Dataset"
              }

            ],
            "output": [
              {
                "name": "winning_model",
                "type": "Model",
                "actions": [
                  "update model",
                  "discard"
                ]
              }
            ],
            "displayIcon": "compare",
            "displayName": "Compare Models"
          }
        ]
      }
    ],
    "services": [
      {
        "name": "active-learning-nodes",
        "panelNames": [
          "dataSplit"
        ],
        "moduleName": "custom_nodes",
        "runtime": {
          "podType": "regular-xs",
          "runnerImage": "gcr.io/viewo-g/piper/agent/runner/cpu/node14:latest",
          "numReplicas": 1,
          "concurrency": 10,
          "pyPackages": {},
          "singleAgent": false,
          "autoscaler": {
            "type": "rabbitmq",
            "minReplicas": 1,
            "maxReplicas": 2,
            "queueLength": 10
          },
          "preemptible": false,
          "executionTimeout": 172800,
          "drainTime": 600,
          "onReset": "failed",
          "runExecutionAsProcess": false
        },
        "maxAttempts": 3
      },
      {
        "name": "model-setup",
        "panelNames": [
          "newModel"
        ],
        "moduleName": "create_new_model",
        "runtime": {
          "podType": "regular-xs",
          "runnerImage": "gcr.io/viewo-g/piper/agent/runner/cpu/node14:latest",
          "numReplicas": 1,
          "concurrency": 10,
          "pyPackages": {},
          "singleAgent": false,
          "autoscaler": {
            "type": "rabbitmq",
            "minReplicas": 1,
            "maxReplicas": 2,
            "queueLength": 10
          },
          "preemptible": false,
          "executionTimeout": 172800,
          "drainTime": 600,
          "onReset": "failed",
          "runExecutionAsProcess": false
        },
        "maxAttempts": 3
      },
      {
        "name": "model-evaluation",
        "panelNames": [
          "evaluateModel"
        ],
        "moduleName": "evaluate_model",
        "runtime": {
          "podType": "highmem-l",
          "runnerImage": "ultralytics/ultralytics:latest",
          "numReplicas": 1,
          "concurrency": 10,
          "pyPackages": {},
          "singleAgent": false,
          "autoscaler": {
            "type": "rabbitmq",
            "minReplicas": 1,
            "maxReplicas": 2,
            "queueLength": 10
          },
          "preemptible": false,
          "executionTimeout": 172800,
          "drainTime": 600,
          "onReset": "failed",
          "runExecutionAsProcess": false
        },
        "maxAttempts": 3
      },
      {
        "name": "model-comparison",
        "panelNames": [
          "compareModels"
        ],
        "moduleName": "compare_models",
        "runtime": {
          "podType": "regular-xs",
          "runnerImage": "gcr.io/viewo-g/piper/agent/runner/cpu/node14:latest",
          "numReplicas": 1,
          "concurrency": 10,
          "pyPackages": {},
          "singleAgent": false,
          "autoscaler": {
            "type": "rabbitmq",
            "minReplicas": 1,
            "maxReplicas": 2,
            "queueLength": 10
          },
          "preemptible": false,
          "executionTimeout": 172800,
          "drainTime": 600,
          "onReset": "failed",
          "runExecutionAsProcess": false
        },
        "maxAttempts": 3
      }
    ]
  }
}